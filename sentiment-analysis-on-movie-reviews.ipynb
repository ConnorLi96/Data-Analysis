{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of sentiment analysis on movie reviews \n",
    "\n",
    "\n",
    "## Introduction of this notebook\n",
    "\n",
    "This project is a machine learning exercise project based on movie reviews. The data set comes from Kaggle. Kaggle divides all reviews into five types. Participants are required to predict the sentiment labels of reviews through text analysis and machine learning model.\n",
    "\n",
    "\n",
    "In this notebook, I mainly use the bag-of-words and TF-IDF model to complete the feature engineering of the text data and then use the logistic regression and Naive Bayesian classifier to train the data and predict the results. And finally, based on the grid search, the optimal combination of super parameters is found.\n",
    "\n",
    "\n",
    "The Kaggle Competition url:\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
    "\n",
    "sentiment labels of film reviews\n",
    "\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive\n",
    "\n",
    "\n",
    "## Feature engineering and ML model\n",
    "\n",
    "steps:\n",
    "\n",
    "- 1. build corpus\n",
    "\n",
    "- 2. bag-of-words\n",
    "    \n",
    "    - Logistic Regression\n",
    "    - Polynomial Naive Bayes\n",
    "    \n",
    "- 3. TF-IDF Model\n",
    "    \n",
    "    - Logistic Regression\n",
    "    - Polynomial Naive Bayes\n",
    "    \n",
    "- 4. Find out the optimal super parameters base on the grid search\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import warnings \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "data_train = pd.read_csv('./train.tsv',sep='\\t') \n",
    "data_test = pd.read_csv('./test.tsv',sep='\\t') \n",
    "\n",
    "# general config\n",
    "stop_words = open('./stop_words.txt',encoding='utf-8').read().splitlines()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head() # train data with sentiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head() # the data set that need to generate sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. build corpus and data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222352,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# contact the text data in senteces \n",
    "train_sentences = data_train['Phrase']\n",
    "test_sentences = data_test['Phrase']\n",
    "sentences = pd.concat([train_sentences,test_sentences])\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the emotional labels \n",
    "label = data_train['Sentiment']\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "co = CountVectorizer(\n",
    "    analyzer='word', # analyzer='word'指的是以词为单位进行分析，对于拉丁语系语言，有时需要以字母'character'为单位进行分析\n",
    "    ngram_range=(1,4), # ngram 指分析相邻的几个词，避免原始的词袋模型中词序丢失的问题\n",
    "    stop_words=stop_words,\n",
    "    max_features=150000 # max_features 指最终的词袋矩阵里面包含语料库中出现次数最多的多少个词\n",
    ")\n",
    "\n",
    "co.fit(sentences)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=1234)\n",
    "\n",
    "# transform words to vector\n",
    "x_train = co.transform(x_train)\n",
    "x_test = co.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Build classifiers and check accuracy, Machine learning and data mining of text processed by word bag model\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature engineering based on bag-of-words，by using Logistic Regression，the accurancy of x_train:\n",
      "0.6461873638344227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg1 = LogisticRegression()\n",
    "lg1.fit(x_train, y_train)\n",
    "print('Text feature engineering based on bag-of-words，by using Logistic Regression，the accurancy of x_train:')\n",
    "print( lg1.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature engineering based on bag-of-words，by using Polynomial Naive Bayes，the accurancy of x_train:\n",
      "0.6084070229398949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train,y_train)\n",
    "print('Text feature engineering based on bag-of-words，by using Polynomial Naive Bayes，the accurancy of x_train:')\n",
    "print(classifier.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1,4),\n",
    "    # stop_words=stop_words,\n",
    "    max_features=150000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0,\n",
       "                max_features=150000, min_df=1, ngram_range=(1, 4), norm='l2',\n",
       "                preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=1234)\n",
    "x_train = tf.transform(x_train)\n",
    "x_test = tf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature engineering based on TF-IDF，by using Polynomial Naive Bayes，the accurancy of x_train:\n",
      "0.6045367166474432\n"
     ]
    }
   ],
   "source": [
    "# 引用朴素贝叶斯进行分类训练和预测\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train,y_train)\n",
    "print('Text feature engineering based on TF-IDF，by using Polynomial Naive Bayes，the accurancy of x_train:')\n",
    "print(classifier.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature engineering based on TF-IDF，by using Logistic Regression，the accurancy of x_train: 0.640958605664488\n",
      "0.640958605664488\n"
     ]
    }
   ],
   "source": [
    "# sklearn 默认的逻辑回归模型\n",
    "\n",
    "lg1 = LogisticRegression()\n",
    "lg1.fit(x_train,y_train)\n",
    "print('Text feature engineering based on TF-IDF，by using Logistic Regression，the accurancy of x_train:',lg1.score(x_test,y_test))\n",
    "print(0.640958605664488)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find out the optimal super parameters base on the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C':range(1,10),\n",
    "             'dual':[True,False]\n",
    "              }\n",
    "lgGS = LogisticRegression()\n",
    "grid = GridSearchCV(lgGS, param_grid=param_grid,cv=3,n_jobs=-1)\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 5, 'dual': True}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the result of super parameter search is that C is 5 and dual is true,\n",
    "# which can make the prediction accuracy of logistic regression model in the verification set the highest. \n",
    "# We then use this optimal parameter to construct the Logistic Regression final classifier, \n",
    "# and finally predict the accuracy of 0.655464 on the verification set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_final = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Through the grid search, find the logical regression model corresponding to the optimal super parameter combination, and predict the accuracy on the verification set:\n",
      "0.6546456491093169\n"
     ]
    }
   ],
   "source": [
    "print('Through the grid search, find the logical regression model corresponding to the optimal super parameter combination, and predict the accuracy on the verification set:')\n",
    "print(lg_final.score(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
